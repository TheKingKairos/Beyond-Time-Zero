{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725850b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_discrete_inference.py\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "DASHBOARD_JSON = Path(\"src/mockData.json\")\n",
    "MODEL_JSON     = Path(\"../../outputs_discrete_xgb/xgb_discrete_model.json\")   # from training script\n",
    "FEATS_PKL      = Path(\"../../outputs_discrete_xgb/xgb_discrete_features.pkl\") # contains {'order':[t_bin]+feats, 'base_features':[...]}\n",
    "OUT_JSON       = Path(\"src/mockData_xgb.json\")                                         # overwrite in place\n",
    "\n",
    "# Your input schema -> rename to match training columns if needed\n",
    "RENAME_MAP = {\n",
    "    \"temp\": \"temperature\",\n",
    "    \"hr\":   \"heartrate\",\n",
    "    # add more renames here if needed\n",
    "}\n",
    "\n",
    "# Discrete-time bin config\n",
    "BIN_MINUTES      = 30.0\n",
    "HORIZON_MINUTES  = 30.0   # per-bin risk you want to rank by (default: next 30m -> t_bin=1)\n",
    "\n",
    "TBIN_COL   = \"t_bin\"\n",
    "EVENT_COL  = \"event\"      # not used in inference\n",
    "ID_COL     = \"stay_id\"    # optional; only for stable sorting if present\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def _coerce_numeric_inplace(df: pd.DataFrame, cols: list[str]) -> None:\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0.0\n",
    "        if pd.api.types.is_bool_dtype(df[c]):\n",
    "            continue\n",
    "        if not pd.api.types.is_numeric_dtype(df[c]):\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        if not pd.api.types.is_bool_dtype(df[c]):\n",
    "            df[c] = df[c].astype(\"float32\")\n",
    "    df[cols] = df[cols].fillna(0)\n",
    "\n",
    "def _compute_tbin(horizon_minutes: float, bin_minutes: float) -> int:\n",
    "    # bin index is 1-based: (0..30] -> 1, (30..60] -> 2, etc.\n",
    "    return int(np.ceil(max(1e-9, horizon_minutes) / bin_minutes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f246a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Load model + features\n",
    "# -----------------------------\n",
    "if not MODEL_JSON.exists() or not FEATS_PKL.exists():\n",
    "    raise FileNotFoundError(\"Trained XGB model or feature list not found. Check paths in Config.\")\n",
    "\n",
    "xgbmodel = xgb.XGBClassifier(device=\"cpu\")\n",
    "xgbmodel.load_model(MODEL_JSON.as_posix())\n",
    "\n",
    "with open(FEATS_PKL, \"rb\") as f:\n",
    "    payload = pickle.load(f)\n",
    "ORDER = payload[\"order\"]            # [t_bin] + base_features (training order)\n",
    "BASE_FEATURES = payload[\"base_features\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e3b1b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Load dashboard data\n",
    "# -----------------------------\n",
    "df = pd.read_json(DASHBOARD_JSON, lines=False)\n",
    "mod_df = df.rename(columns=RENAME_MAP).copy()\n",
    "\n",
    "# Ensure all model features exist\n",
    "for c in BASE_FEATURES:\n",
    "    if c not in mod_df.columns:\n",
    "        mod_df[c] = 0.0  # missing -> 0; change if you prefer another imputation\n",
    "\n",
    "# Coerce numerics\n",
    "_coerce_numeric_inplace(mod_df, BASE_FEATURES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "249cd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build desired time bin (per-bin, not cumulative)\n",
    "tbin = _compute_tbin(HORIZON_MINUTES, BIN_MINUTES)\n",
    "G = mod_df[BASE_FEATURES].copy()\n",
    "G.insert(0, TBIN_COL, np.full(len(G), tbin, dtype=np.int16))\n",
    "\n",
    "# Ensure column order matches training\n",
    "need_order = [TBIN_COL] + BASE_FEATURES\n",
    "if ORDER and isinstance(ORDER, list):\n",
    "    # Training saved order takes precedence\n",
    "    need_order = ORDER\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed7a67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([77.5       , 68.69999695, 32.79999924, 46.59999847, 89.6000061 ,\n",
       "       32.20000076, 46.        , 31.09999847,  3.5999999 , 36.09999847])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict per-bin risk (probability of event in this bin)\n",
    "probs = xgbmodel.predict_proba(G[need_order])[:, 1]\n",
    "probs_pct = (np.round(probs, 3) * 100).astype(float)\n",
    "probs_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a08cc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated src/mockData_xgb.json\n",
      "   Used per-bin risk at horizon=30.0 min (t_bin=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Update dashboard fields\n",
    "# -----------------------------\n",
    "# Example: keep your previous ensemble score if you want, but here we\n",
    "# add a pure XGB per-bin risk and rank by it.\n",
    "df[\"xgbRiskPct_bin\"] = probs_pct\n",
    "df[\"xgbRisk_bin_idx\"] = tbin  # which bin this probability refers to\n",
    "df[\"priorityRank\"] = df[\"xgbRiskPct_bin\"].rank(ascending=False, method=\"first\").astype(int)\n",
    "\n",
    "# (Optional) Keep your trends updated like in your Cox code\n",
    "for i in range(len(df)):\n",
    "    # Defensive guards in case the structure varies\n",
    "    try:\n",
    "        df.at[i, \"trends\"][-1][\"temp\"] = df.at[i, \"temp\"]\n",
    "        df.at[i, \"trends\"][-1][\"heartRate\"] = df.at[i, \"hr\"]\n",
    "        if \"lactate\" in df.columns:\n",
    "            df.at[i, \"trends\"][-1][\"lactate\"] = df.at[i, \"lactate\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Sort by priority and write back\n",
    "#df = df.sort_values(by=\"priorityRank\", kind=\"mergesort\").reset_index(drop=True)\n",
    "df.to_json(OUT_JSON, orient=\"records\", lines=False, indent=1)\n",
    "\n",
    "print(f\"✅ Updated {OUT_JSON}\")\n",
    "print(f\"   Used per-bin risk at horizon={HORIZON_MINUTES} min (t_bin={tbin})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7edba51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sepsis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
