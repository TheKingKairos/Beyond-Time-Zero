from __future__ import annotations
import os
import io
import json
from pathlib import Path
import numpy as np
import pandas as pd
import pickle
import warnings

from sklearn.model_selection import GroupShuffleSplit
from sklearn.preprocessing import StandardScaler
from lifelines import CoxPHFitter, CoxTimeVaryingFitter
from lifelines.utils import concordance_index
from lifelines.statistics import proportional_hazard_test

# NEW: plotting backend (headless-safe)
import matplotlib
matplotlib.use("Agg")  # ensure we can save figures without a display
import matplotlib.pyplot as plt

# NEW: optional metrics from scikit-survival
try:
    from sksurv.metrics import brier_score, integrated_brier_score
    from sksurv.util import Surv
    HAVE_SKSURV = True
except ImportError:
    HAVE_SKSURV = False
    print("[NOTE] scikit-survival not installed; Brier / IBS metrics will be skipped.")

from lifelines.exceptions import StatError



# ----------------------------------------------------
# Config
# ----------------------------------------------------
DATA_DIR = Path("data/MIMIC-ED")
STATIC_PATH = DATA_DIR / "cox_static_random_train.csv"
STACKED_PATH = DATA_DIR / "cox_static_landmark_stacked_train.csv"  # generated by your formatter
TVC_PATH    = DATA_DIR / "cox_timevarying_train.csv"   # generated by the optional step in your formatter

OUT_DIR = Path("outputs_cox")
OUT_DIR.mkdir(parents=True, exist_ok=True)
PLOTS_DIR = OUT_DIR / "plots"
PLOTS_DIR.mkdir(parents=True, exist_ok=True)

# Column names
DURATION_COL = "duration"
EVENT_COL    = "event"
ID_COL_TVC   = "stay_id"   # id column present in time-varying data
ID_COL_STATIC = "stay_id"
START_COL    = "start"
STOP_COL     = "stop"

RNG = 42
TEST_SIZE = 0.2
PENALIZER_STATIC = 0.1
PENALIZER_TVC    = 0.1

# How many example individuals to plot
N_EX_SURV = 10

# ----------------------------------------------------
# Utilities
# ----------------------------------------------------
def _numeric_feature_cols(df: pd.DataFrame, exclude: list[str]) -> list[str]:
    num = df.select_dtypes(include=[np.number]).columns.tolist()
    cols = [c for c in num if c not in exclude]
    return [c for c in cols if df[c].std(ddof=0) > 0]

def _save_summary(df_summary: pd.DataFrame, path: Path) -> None:
    df_summary.to_csv(path, index=True)

def _print_metrics(title: str, tr_ci: float, te_ci: float, n_tr: int, n_te: int) -> None:
    print(f"\n[{title}]  n_train={n_tr}  n_test={n_te}")
    print(f"  Train C-index: {tr_ci:.4f}")
    print(f"  Test  C-index: {te_ci:.4f}")

def _fit_and_apply_scaler(tr_df: pd.DataFrame, te_df: pd.DataFrame, feats: list[str]) -> StandardScaler:
    for c in feats:
        tr_df[c] = tr_df[c].astype("float64", copy=False)
        te_df[c] = te_df[c].astype("float64", copy=False)

    scaler = StandardScaler()
    X_tr = scaler.fit_transform(tr_df[feats].to_numpy(dtype="float64"))
    X_te = scaler.transform(te_df[feats].to_numpy(dtype="float64"))

    for j, c in enumerate(feats):
        tr_df[c] = X_tr[:, j]
        te_df[c] = X_te[:, j]

    return scaler

def _save_scaler(scaler: StandardScaler, feats: list[str], name: str) -> None:
    payload = {"features": feats, "scaler": scaler}
    with open(OUT_DIR / f"{name}_scaler.pkl", "wb") as f:
        pickle.dump(payload, f)

def _safe_close(fig):
    try:
        plt.close(fig)
    except Exception:
        pass

def _save_current_fig(path: Path):
    plt.tight_layout()
    plt.savefig(path, dpi=150, bbox_inches="tight")
    plt.close()

def _save_fig(fig, path: Path):
    fig.tight_layout()
    fig.savefig(path, dpi=150, bbox_inches="tight")
    _safe_close(fig)

def _extra_likelihood_metrics(fitter, model_type: str = "auto") -> dict:
    """
    Collect likelihood-based metrics without crashing if the attribute
    raises (e.g., CoxPHFitter.AIC_).

    model_type:
      - "cox"        : use AIC_partial_ / BIC_partial_ if available
      - "parametric" : use AIC_ / BIC_ (for AFT models)
      - "auto"       : infer from class name
    """
    # Infer type if requested
    if model_type == "auto":
        name = fitter.__class__.__name__.lower()
        if "coxph" in name or "coxtimevarying" in name or "coxtimevaryingfitter" in name:
            model_type = "cox"
        else:
            model_type = "parametric"

    def _safe_get(attr: str):
        try:
            return getattr(fitter, attr)
        except (AttributeError, StatError, Exception):
            return None

    out = {}

    # Log-likelihood is common
    ll = _safe_get("log_likelihood_")
    if ll is not None:
        out["log_likelihood"] = float(ll)

    if model_type == "cox":
        # semi-parametric: use partial AIC/BIC
        aic = _safe_get("AIC_partial_")
        bic = _safe_get("BIC_partial_")
    else:
        # fully parametric: use standard AIC/BIC
        aic = _safe_get("AIC_")
        bic = _safe_get("BIC_")

    if aic is not None:
        out["AIC"] = float(aic)
    if bic is not None:
        out["BIC"] = float(bic)

    return out



def _time_grid_for_brier(tr: pd.DataFrame, n_times: int = 25) -> np.ndarray:
    """
    Choose a reasonable grid of evaluation times within the training range.
    """
    t = tr[DURATION_COL].to_numpy(dtype="float64")
    t = t[t > 0]
    if len(t) == 0:
        return np.array([], dtype="float64")
    t_min = float(np.percentile(t, 5))
    t_max = float(np.percentile(t, 90))
    if t_max <= t_min:
        t_min, t_max = float(t.min()), float(t.max())
    return np.linspace(t_min, t_max, n_times)


def _survival_metrics_sksurv(
    fitter,
    tr: pd.DataFrame,
    te: pd.DataFrame,
    feats: list[str],
    *,
    duration_col: str = DURATION_COL,
    event_col: str = EVENT_COL,
    prefix: str = "",
) -> dict:
    """
    Compute Brier(t) and integrated Brier score on test set via scikit-survival.

    Returns a dict with keys like:
      f"{prefix}brier_times", f"{prefix}brier_scores", f"{prefix}ibs"
    or {} if scikit-survival is not available.
    """
    if not HAVE_SKSURV:
        return {}

    # build structured arrays (event: bool, time: float)
    y_tr = Surv.from_arrays(
        event=tr[event_col].astype(bool).to_numpy(),
        time=tr[duration_col].astype("float64").to_numpy(),
    )
    y_te = Surv.from_arrays(
        event=te[event_col].astype(bool).to_numpy(),
        time=te[duration_col].astype("float64").to_numpy(),
    )

    times = _time_grid_for_brier(tr)
    if times.size == 0:
        return {}

    # lifelines: predict_survival_function(X, times) -> DataFrame (index: time, columns: samples)
    surv_te_df = fitter.predict_survival_function(te[feats], times=times)
    # scikit-survival expects shape (n_test, n_times)
    surv_te = surv_te_df.T.to_numpy(dtype="float64")

    # time-dependent Brier score at each time
    bs_times, bs_scores = brier_score(y_tr, y_te, surv_te, times)

    # integrated Brier score over the grid
    ibs_val = integrated_brier_score(y_tr, y_te, surv_te, times)

    return {
        f"{prefix}brier_times": bs_times.astype(float).tolist(),
        f"{prefix}brier_scores": bs_scores.astype(float).tolist(),
        f"{prefix}ibs": float(ibs_val),
    }


# ----------------------------------------------------
# Plotting & diagnostics (NEW)
# ----------------------------------------------------
def _plot_cph_coefficients(cph: CoxPHFitter, name: str):
    fig = cph.plot()
    _save_fig(fig.figure, PLOTS_DIR / f"{name}_coefficients.png")

def _plot_baselines_common(fitter, name: str):
    # baseline survival
    plt.figure()
    fitter.baseline_survival_.plot(legend=False)
    plt.title(f"{name}: Baseline Survival Function")
    plt.xlabel("Time")
    plt.ylabel("S0(t)")
    _save_current_fig(PLOTS_DIR / f"{name}_baseline_survival.png")

    # baseline cumulative hazard
    plt.figure()
    fitter.baseline_cumulative_hazard_.plot(legend=False)
    plt.title(f"{name}: Baseline Cumulative Hazard")
    plt.xlabel("Time")
    plt.ylabel("H0(t)")
    _save_current_fig(PLOTS_DIR / f"{name}_baseline_cumhaz.png")

def _plot_cph_individual_curves(cph: CoxPHFitter, df: pd.DataFrame, feats: list[str], name: str, which: str):
    """
    Plot survival and cumulative hazard curves for a few individuals (CoxPH only).
    """
    if len(df) == 0:
        return
    rng = np.random.default_rng(RNG)
    sample_idx = rng.choice(len(df), size=min(N_EX_SURV, len(df)), replace=False)
    X = df.iloc[sample_idx][feats]

    # survival curves
    survs = cph.predict_survival_function(X)
    plt.figure()
    for col in survs.columns:
        plt.plot(survs.index.values, survs[col].values, alpha=0.5)
    plt.title(f"{name} ({which}): Individual Survival Curves")
    plt.xlabel("Time")
    plt.ylabel("S(t | x)")
    _save_current_fig(PLOTS_DIR / f"{name}_{which}_survival_individuals.png")

    # cumulative hazards
    cumhaz = cph.predict_cumulative_hazard(X)
    #omit one with largest values since it is an outlier
    cumhaz = cumhaz.drop(columns=cumhaz.sum().idxmax())
    plt.figure()
    for col in cumhaz.columns:
        plt.plot(cumhaz.index.values, cumhaz[col].values, alpha=0.5)
    plt.title(f"{name} ({which}): Individual Cumulative Hazards")
    plt.xlabel("Time")
    plt.ylabel("H(t | x)")
    _save_current_fig(PLOTS_DIR / f"{name}_{which}_cumhaz_individuals.png")

def _plot_cph_individual_curves_stag(
    cph: CoxPHFitter,
    df: pd.DataFrame,
    feats: list[str],
    name: str,
    which: str,
    *,
    stagger: bool = True,
    stagger_frac: float = 0.20,   # max shift as fraction of time window
    vline_at: float | None = None # position for vertical comparison line
):
    """
    Plot survival and cumulative hazard curves for a few individuals (CoxPH only).
    Adds staggered start times and a red vertical comparison line.
    Zooms in to [0, 20] on the time axis.
    """
    if len(df) == 0:
        return

    rng = np.random.default_rng(RNG)
    sample_idx = rng.choice(len(df), size=min(N_EX_SURV, len(df)), replace=False)
    X = df.iloc[sample_idx][feats]
    df_sample = df.iloc[sample_idx]

    # ------------------------------------------------
    # Cumulative hazard curves (staggered + vline)
    # ------------------------------------------------
    cumhaz = cph.predict_cumulative_hazard(X)
    #omit one with largest values since it is an outlier
    cumhaz = cumhaz.drop(columns=cumhaz.sum().idxmax())
    t = cumhaz.index.values
    if len(t) == 0:
        return

    max_t = float(np.max(t))
    if vline_at is None:
        vline_at = 0.75 * max_t
    # constrain vline to [0, 15]
    vline_at = float(np.clip(vline_at, 0, 15))

    # create per-curve offsets
    if stagger and max_t > 0:
        if DURATION_COL in df_sample.columns:
            durs = df_sample[DURATION_COL].to_numpy(dtype="float64")
            ranks = pd.Series(durs).rank(method="first").to_numpy()
            norm = (ranks - ranks.min()) / (ranks.max() - ranks.min() + 1e-12)
        else:
            norm = np.linspace(0.0, 1.0, num=len(cumhaz.columns), endpoint=True)
        offsets = norm * (stagger_frac * max_t)
    else:
        offsets = np.zeros(len(cumhaz.columns), dtype="float64")

    plt.figure()
    for j, col in enumerate(cumhaz.columns):
        plt.plot(t + offsets[j], cumhaz[col].values, alpha=0.5)
    plt.axvline(vline_at, color="red", linestyle="--", linewidth=1.2, alpha=0.9)
    plt.title(f"{name} ({which}): Individual Cumulative Hazards (staggered)")
    plt.xlabel("Time")
    plt.ylabel("H(t | x)")
    plt.xlim(0, 20)  # zoom into early time window
    _save_current_fig(PLOTS_DIR / f"{name}_{which}_cumhaz_individuals_stag.png")



def _plot_ctv_coefficients(ctv: CoxTimeVaryingFitter, name: str):
    # lifelines fitters implement .plot() for coefficients
    fig = ctv.plot()
    _save_fig(fig.figure, PLOTS_DIR / f"{name}_coefficients.png")

def _ph_assumption_report_and_plots_cph(cph: CoxPHFitter, df: pd.DataFrame, feats: list[str], name: str):
    """
    Run PH tests, save CSV of p-values, save textual report, and dump figures produced
    by check_assumptions (scaled Schoenfeld residuals, etc.).
    """
    cols = [DURATION_COL, EVENT_COL] + feats
    df_use = df[cols].copy()

    # 1) proportional_hazard_test summary (CSV)
    ph_test = proportional_hazard_test(cph, df_use, time_transform="rank")
    ph_res = ph_test.summary  # DataFrame with p/chi2 etc.
    ph_res.to_csv(OUT_DIR / f"{name}_ph_test_summary.csv", index=True)

    # 2) lifelines' convenience checker with plots; capture stdout + figures
    # NOTE: assert_p_value=False avoids raising if something fails thresholds.
    with io.StringIO() as buf:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            # capture figures created by check_assumptions
            before = set(plt.get_fignums())
            try:
                # verbose text goes to stdout; we capture it
                from contextlib import redirect_stdout
                with redirect_stdout(buf):
                    cph.check_assumptions(
                        df_use,
                        p_value_threshold=0.05,
                        show_plots=True,
                    )
            except Exception as e:
                # ensure we still save whatever we can
                print(f"[check_assumptions warning] {e}", file=buf)
            text = buf.getvalue()
        # Save textual report
        (OUT_DIR / f"{name}_ph_check_report.txt").write_text(text)

    # Save any new figs created by check_assumptions
    after = set(plt.get_fignums())
    new_nums = sorted(list(after - before))
    for i, num in enumerate(new_nums):
        fig = plt.figure(num)
        fig_path = PLOTS_DIR / f"{name}_ph_diag_{i+1}.png"
        _save_fig(fig, fig_path)

# ----------------------------------------------------
# Plain CoxPH (static)
# ----------------------------------------------------
def run_static_cox(static_path: Path) -> dict:
    df = pd.read_csv(static_path)

    # Basic checks
    for c in [DURATION_COL, EVENT_COL]:
        if c not in df.columns:
            raise ValueError(f"Required column '{c}' missing from {static_path}")
    if (df[DURATION_COL] <= 0).any():
        raise ValueError("Non-positive durations found in static Cox data.")
    if not set(df[EVENT_COL].unique()).issubset({0, 1}):
        raise ValueError("Event column must be binary (0/1).")

    # features
    exclude = [DURATION_COL, EVENT_COL]
    if ID_COL_STATIC in df.columns:
        exclude.append(ID_COL_STATIC)
    feats = _numeric_feature_cols(df, exclude=exclude)

    # split
    if ID_COL_STATIC in df.columns:
        gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=RNG)
        idx = np.arange(len(df))
        tr_idx, te_idx = next(gss.split(idx, groups=df[ID_COL_STATIC].values))
        tr, te = df.iloc[tr_idx].copy(), df.iloc[te_idx].copy()
        n_tr_group = tr[ID_COL_STATIC].nunique()
        n_te_group = te[ID_COL_STATIC].nunique()
    else:
        gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=RNG)
        idx = np.arange(len(df))
        tr_idx, te_idx = next(gss.split(idx, groups=idx))
        tr, te = df.iloc[tr_idx].copy(), df.iloc[te_idx].copy()
        n_tr_group = len(tr)
        n_te_group = len(te)

    # scale
    scaler = _fit_and_apply_scaler(tr, te, feats)
    _save_scaler(scaler, feats, name="coxph_static")

    # fit
    cph = CoxPHFitter(penalizer=PENALIZER_STATIC)
    cph.fit(tr[[DURATION_COL, EVENT_COL] + feats],
            duration_col=DURATION_COL, event_col=EVENT_COL, show_progress=False)

    # metrics
    tr_scores = cph.predict_partial_hazard(tr).values.ravel()
    te_scores = cph.predict_partial_hazard(te).values.ravel()
    tr_ci = concordance_index(tr[DURATION_COL], tr_scores, tr[EVENT_COL])
    te_ci = concordance_index(te[DURATION_COL], te_scores, te[EVENT_COL])


    # NEW: likelihood-based metrics and Brier/IBS
    extra_metrics = {}
    extra_metrics.update(_extra_likelihood_metrics(cph, model_type="cox"))
    extra_metrics.update(
        _survival_metrics_sksurv(
            cph,
            tr,
            te,
            feats,
            prefix="coxph_static_",
        )
    )


    _save_summary(cph.summary, OUT_DIR / "coxph_static_summary.csv")
    pickle_save(cph, "coxph_static")

    print(f"\n[CoxPH (static/stacked)]  n_train={len(tr)}  n_test={len(te)}  "
          f"(train_stays={n_tr_group}, test_stays={n_te_group})")
    print(f"  Train C-index: {tr_ci:.4f}")
    print(f"  Test  C-index: {te_ci:.4f}")

    if "log_likelihood" in extra_metrics:
        print(f"  Log-likelihood (train): {extra_metrics['log_likelihood']:.3f}")
    if "AIC" in extra_metrics:
        print(f"  AIC (train): {extra_metrics['AIC']:.3f}")
    if "BIC" in extra_metrics:
        print(f"  BIC (train): {extra_metrics['BIC']:.3f}")
    if "coxph_static_ibs" in extra_metrics:
        print(f"  Test IBS: {extra_metrics['coxph_static_ibs']:.4f}")


    # ===== NEW: plots & PH assumption diagnostics =====
    _plot_cph_coefficients(cph, "Cox Proportional Hazards Model")
    _plot_baselines_common(cph, "Cox Proportional Hazards Model")
    #_plot_cph_individual_curves(cph, tr, feats, "Cox Proportional Hazards Model", "train")
    _plot_cph_individual_curves(cph, te, feats, "Cox Proportional Hazards Model", "test")
    #_plot_cph_individual_curves_stag(cph, tr, feats, "Cox Proportional Hazards Model", "train", stagger=True, vline_at=5)
    _plot_cph_individual_curves_stag(cph, te, feats, "Cox Proportional Hazards Model", "test", stagger=True, vline_at=5)
    #_ph_assumption_report_and_plots_cph(cph, tr[[DURATION_COL, EVENT_COL] + feats], feats, "Cox Proportional Hazards Model")

    return {
        "train_cindex": float(tr_ci),
        "test_cindex": float(te_ci),
        "n_train": int(len(tr)),
        "n_test": int(len(te)),
        "n_train_groups": int(n_tr_group),
        "n_test_groups": int(n_te_group),
        "n_features": int(len(feats)),
        **extra_metrics,
    }


def run_static_cox_stacked(stacked_path: Path) -> dict:
    df = pd.read_csv(stacked_path)

    # Basic checks
    for c in [DURATION_COL, EVENT_COL, ID_COL_STATIC]:
        if c not in df.columns:
            raise ValueError(f"Required column '{c}' missing from {stacked_path}")
    if (df[DURATION_COL] <= 0).any():
        raise ValueError("Non-positive durations found in static Cox data.")
    if not set(df[EVENT_COL].unique()).issubset({0, 1}):
        raise ValueError("Event column must be binary (0/1).")

    # features
    exclude = [DURATION_COL, EVENT_COL]
    if ID_COL_STATIC in df.columns:
        exclude.append(ID_COL_STATIC)
    feats = _numeric_feature_cols(df, exclude=exclude)

    # split
    if ID_COL_STATIC in df.columns:
        gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=RNG)
        idx = np.arange(len(df))
        tr_idx, te_idx = next(gss.split(idx, groups=df[ID_COL_STATIC].values))
        tr, te = df.iloc[tr_idx].copy(), df.iloc[te_idx].copy()
        n_tr_group = tr[ID_COL_STATIC].nunique()
        n_te_group = te[ID_COL_STATIC].nunique()
    else:
        gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=RNG)
        idx = np.arange(len(df))
        tr_idx, te_idx = next(gss.split(idx, groups=idx))
        tr, te = df.iloc[tr_idx].copy(), df.iloc[te_idx].copy()
        n_tr_group = len(tr)
        n_te_group = len(te)

    # scale
    scaler = _fit_and_apply_scaler(tr, te, feats)
    _save_scaler(scaler, feats, name="coxph_static")

    # fit
    cph = CoxPHFitter(penalizer=PENALIZER_STATIC)
    cph.fit(tr[[DURATION_COL, EVENT_COL] + feats],
            duration_col=DURATION_COL, event_col=EVENT_COL, show_progress=False)

    # metrics
    tr_scores = cph.predict_partial_hazard(tr).values.ravel()
    te_scores = cph.predict_partial_hazard(te).values.ravel()
    tr_ci = concordance_index(tr[DURATION_COL], tr_scores, tr[EVENT_COL])
    te_ci = concordance_index(te[DURATION_COL], te_scores, te[EVENT_COL])

    _save_summary(cph.summary, OUT_DIR / "coxph_static_summary.csv")
    pickle_save(cph, "coxph_static")

    print(f"\n[CoxPH (static/stacked)]  n_train={len(tr)}  n_test={len(te)}  "
          f"(train_stays={n_tr_group}, test_stays={n_te_group})")
    print(f"  Train C-index: {tr_ci:.4f}")
    print(f"  Test  C-index: {te_ci:.4f}")

    # ===== NEW: plots & PH assumption diagnostics =====
    _plot_cph_coefficients(cph, "coxph_static")
    _plot_baselines_common(cph, "coxph_static")
    _plot_cph_individual_curves(cph, tr, feats, "coxph_static", "train")
    _plot_cph_individual_curves(cph, te, feats, "coxph_static", "test")
    #_ph_assumption_report_and_plots_cph(cph, tr[[DURATION_COL, EVENT_COL] + feats], feats, "coxph_static")

    return {
        "train_cindex": float(tr_ci),
        "test_cindex": float(te_ci),
        "n_train": int(len(tr)),
        "n_test": int(len(te)),
        "n_train_groups": int(n_tr_group),
        "n_test_groups": int(n_te_group),
        "n_features": int(len(feats)),
    }

# ----------------------------------------------------
# Time-varying Cox (Andersen–Gill)
# ----------------------------------------------------
def run_timevarying_cox(tvc_path: Path) -> dict:
    if not tvc_path.exists():
        raise FileNotFoundError(
            f"Missing {tvc_path}. Generate it with your formatter (cox_timevarying.csv) "
            f"(uncomment the TVC step or write it out explicitly)."
        )
    df = pd.read_csv(tvc_path)

    needed = {ID_COL_TVC, START_COL, STOP_COL, EVENT_COL}
    missing = needed - set(df.columns)
    if missing:
        raise ValueError(f"Time-varying data missing columns: {missing}")
    if (df[STOP_COL] <= df[START_COL]).any():
        raise ValueError("Found non-positive interval lengths in time-varying data.")
    if not set(df[EVENT_COL].unique()).issubset({0, 1}):
        raise ValueError("Event column must be binary (0/1).")

    # features: numeric excluding id/start/stop/event
    feats = _numeric_feature_cols(df, exclude=[EVENT_COL])
    feats = [c for c in feats if c not in {ID_COL_TVC, START_COL, STOP_COL}]

    # split by IDs
    unique_ids = df[ID_COL_TVC].dropna().unique()
    rng = np.random.default_rng(RNG)
    rng.shuffle(unique_ids)
    n_test = int(np.ceil(TEST_SIZE * len(unique_ids)))
    test_ids = set(unique_ids[:n_test])
    train_ids = set(unique_ids[n_test:])

    tr = df[df[ID_COL_TVC].isin(train_ids)].copy()
    te = df[df[ID_COL_TVC].isin(test_ids)].copy()

    # scale
    scaler = _fit_and_apply_scaler(tr, te, feats)
    _save_scaler(scaler, feats, name="cox_tvc")

    # fit
    ctv = CoxTimeVaryingFitter(penalizer=PENALIZER_TVC)
    ctv.fit(tr[[ID_COL_TVC, START_COL, STOP_COL, EVENT_COL] + feats],
            id_col=ID_COL_TVC, start_col=START_COL, stop_col=STOP_COL, event_col=EVENT_COL,
            show_progress=False)

    # metrics (use stop as "time")
    tr_scores = ctv.predict_partial_hazard(tr).values.ravel()
    te_scores = ctv.predict_partial_hazard(te).values.ravel()
    tr_ci = concordance_index(tr[STOP_COL], tr_scores, tr[EVENT_COL])
    te_ci = concordance_index(te[STOP_COL], te_scores, te[EVENT_COL])

    _save_summary(ctv.summary, OUT_DIR / "cox_tvc_summary.csv")
    _print_metrics("CoxTimeVarying (start/stop)", tr_ci, te_ci, tr[ID_COL_TVC].nunique(), te[ID_COL_TVC].nunique())

    pickle_save(ctv, "cox_tvc")

    # ===== NEW: plots for TVC (no PH test here) =====
    _plot_ctv_coefficients(ctv, "cox_tvc")
    _plot_baselines_common(ctv, "cox_tvc")

    # (Optional) If you want example predicted cumulative hazards by interval,
    # lifelines allows predict_cumulative_hazard on the long format as well.
    # We'll provide a small sample to visualize:
    try:
        sample = te.sample(n=min(N_EX_SURV, len(te)), random_state=RNG)
        ch = ctv.predict_cumulative_hazard(sample)
        # multiple columns per row; plot them lightly
        plt.figure()
        for col in ch.columns[:min(N_EX_SURV, ch.shape[1])]:
            plt.plot(ch.index.values, ch[col].values, alpha=0.5)
        plt.title("cox_tvc (test): Example Cumulative Hazards")
        plt.xlabel("Time")
        plt.ylabel("H(t | x(t))")
        _save_current_fig(PLOTS_DIR / "cox_tvc_test_cumhaz_examples.png")
    except Exception as e:
        print(f"[WARN] Could not plot individual cumulative hazards for TVC: {e}")

    return {
        "train_cindex": float(tr_ci),
        "test_cindex": float(te_ci),
        "n_train_ids": int(tr[ID_COL_TVC].nunique()),
        "n_test_ids": int(te[ID_COL_TVC].nunique()),
        "n_features": int(len(feats)),
    }

# ----------------------------------------------------
# Pickling helpers
# ----------------------------------------------------
def pickle_save(fitter, name: str) -> None:
    with open(OUT_DIR / f"{name}.pkl", "wb") as f:
        pickle.dump(fitter, f)

def pickle_load(path: str):
    with open(path, "rb") as f:
        model = pickle.load(f)
    if not isinstance(model, (CoxPHFitter, CoxTimeVaryingFitter)):
        raise TypeError(f"Unexpected model type: {type(model)}")
    return model

def load_coxph(path: str) -> CoxPHFitter:
    m = pickle_load(path)
    if not isinstance(m, CoxPHFitter):
        raise TypeError(f"Expected CoxPHFitter, got {type(m)}")
    return m

def load_coxtv(path: str) -> CoxTimeVaryingFitter:
    m = pickle_load(path)
    if not isinstance(m, CoxTimeVaryingFitter):
        raise TypeError(f"Expected CoxTimeVaryingFitter, got {type(m)}")
    return m

# ----------------------------------------------------
# Parametric Survival (AFT) models on static/stacked data
# ----------------------------------------------------
def _import_parametric_fitters():
    """
    Try to import regression-capable parametric fitters.
    Some are version-dependent; we skip models that aren't present.
    """
    available = {}

    # Weibull / LogNormal / LogLogistic AFT are standard
    try:
        from lifelines import WeibullAFTFitter
        available["weibull"] = WeibullAFTFitter
    except Exception:
        pass
    try:
        from lifelines import LogNormalAFTFitter
        available["lognormal"] = LogNormalAFTFitter
    except Exception:
        pass
    try:
        from lifelines import LogLogisticAFTFitter
        available["loglogistic"] = LogLogisticAFTFitter
    except Exception:
        pass

    # Exponential AFT (if your lifelines has it)
    # try:
    #     from lifelines import ExponentialAFTFitter
    #     available["exponential"] = ExponentialAFTFitter
    # except Exception:
    #     # Some lifelines versions don't expose an Exponential AFT regression fitter.
    #     print("[NOTE] ExponentialAFTFitter not found; skipping the exponential regression model.")
    #     pass

    # Generalized Gamma regression (version-dependent)
    # # lifelines historically had GeneralizedGammaRegressionFitter in newer versions.
    # try:
    #     from lifelines import GeneralizedGammaRegressionFitter
    #     available["gengamma"] = GeneralizedGammaRegressionFitter
    # except Exception:
    #     print("[NOTE] GeneralizedGammaRegressionFitter not found; skipping generalized gamma regression.")
    #     pass

    return available


def _plot_aft_coefficients(fitter, name: str):
    try:
        fig = fitter.plot()  # lifelines fitters implement .plot() for coefficients
        _save_fig(fig.figure if hasattr(fig, "figure") else fig, PLOTS_DIR / f"{name}_coefficients.png")
    except Exception as e:
        print(f"[WARN] Could not plot coefficients for {name}: {e}")


def _plot_parametric_baselines(fitter, name: str):
    # baseline survival
    try:
        plt.figure()
        fitter.baseline_survival_.plot(legend=False)
        plt.title(f"{name}: Baseline Survival Function")
        plt.xlabel("Time")
        plt.ylabel("S0(t)")
        _save_current_fig(PLOTS_DIR / f"{name}_baseline_survival.png")
    except Exception as e:
        print(f"[WARN] Could not plot baseline survival for {name}: {e}")

    # baseline cumulative hazard
    try:
        plt.figure()
        fitter.baseline_cumulative_hazard_.plot(legend=False)
        plt.title(f"{name}: Baseline Cumulative Hazard")
        plt.xlabel("Time")
        plt.ylabel("H0(t)")
        _save_current_fig(PLOTS_DIR / f"{name}_baseline_cumhaz.png")
    except Exception as e:
        print(f"[WARN] Could not plot baseline cumulative hazard for {name}: {e}")


def _plot_aft_individual_curves(fitter, df: pd.DataFrame, feats: list[str], name: str, which: str):
    if len(df) == 0:
        return
    rng = np.random.default_rng(RNG)
    sample_idx = rng.choice(len(df), size=min(N_EX_SURV, len(df)), replace=False)
    X = df.iloc[sample_idx][feats]

    # survival curves
    try:
        survs = fitter.predict_survival_function(X)
        plt.figure()
        for col in survs.columns:
            plt.plot(survs.index.values, survs[col].values, alpha=0.5)
        plt.title(f"{name} ({which}): Individual Survival Curves")
        plt.xlabel("Time")
        plt.ylabel("S(t | x)")
        _save_current_fig(PLOTS_DIR / f"{name}_{which}_survival_individuals.png")
    except Exception as e:
        print(f"[WARN] Could not plot survival curves for {name} ({which}): {e}")

    # cumulative hazard
    try:
        cumhaz = fitter.predict_cumulative_hazard(X)
        plt.figure()
        for col in cumhaz.columns:
            plt.plot(cumhaz.index.values, cumhaz[col].values, alpha=0.5)
        plt.title(f"{name} ({which}): Individual Cumulative Hazards")
        plt.xlabel("Time")
        plt.ylabel("H(t | x)")
        _save_current_fig(PLOTS_DIR / f"{name}_{which}_cumhaz_individuals.png")
    except Exception as e:
        print(f"[WARN] Could not plot cumulative hazards for {name} ({which}): {e}")


def _aft_risk_scores_from_median(fitter, X: pd.DataFrame) -> np.ndarray:
    """
    For AFT models, use negative predicted median survival time as a risk score.
    Shorter predicted survival => higher risk.
    """
    preds = fitter.predict_median(X)  # returns a pandas Series of times
    # convert to numpy; negative for "higher score => higher risk"
    return -preds.to_numpy(dtype="float64")


def _save_parametric_summary(fitter, path: Path):
    try:
        # AFT fitters expose a .summary DataFrame with params and CIs
        summary = fitter.summary
        if isinstance(summary, pd.DataFrame):
            summary.to_csv(path, index=True)
        else:
            # fallback: try to coerce to DataFrame if possible
            pd.DataFrame(summary).to_csv(path, index=True)
    except Exception as e:
        print(f"[WARN] Could not write summary CSV to {path}: {e}")


def run_parametric_aft_models(static_path: Path) -> dict:
    """
    Fit multiple parametric regression models on the static dataset:
      - WeibullAFTFitter
      - LogNormalAFTFitter
      - LogLogisticAFTFitter
      - ExponentialAFTFitter (if available)
      - GeneralizedGammaRegressionFitter (if available)

    Returns dict keyed by model name with metrics & counts.
    """
    # --------------------------------------------
    # Load/check data & select numeric features
    # --------------------------------------------
    df = pd.read_csv(static_path)

    for c in [DURATION_COL, EVENT_COL]:
        if c not in df.columns:
            raise ValueError(f"Required column '{c}' missing from {static_path}")
    if (df[DURATION_COL] <= 0).any():
        raise ValueError("Non-positive durations found in static data.")
    if not set(df[EVENT_COL].unique()).issubset({0, 1}):
        raise ValueError("Event column must be binary (0/1).")

    exclude = [DURATION_COL, EVENT_COL]
    if ID_COL_STATIC in df.columns:
        exclude.append(ID_COL_STATIC)
    feats = _numeric_feature_cols(df, exclude=exclude)
    if len(feats) == 0:
        raise ValueError("No numeric covariates found for parametric regression models.")

    # --------------------------------------------
    # Train/test split (group-aware if stay_id present)
    # --------------------------------------------
    if ID_COL_STATIC in df.columns:
        gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=RNG)
        idx = np.arange(len(df))
        tr_idx, te_idx = next(gss.split(idx, groups=df[ID_COL_STATIC].values))
        tr, te = df.iloc[tr_idx].copy(), df.iloc[te_idx].copy()
        n_tr_group = tr[ID_COL_STATIC].nunique()
        n_te_group = te[ID_COL_STATIC].nunique()
    else:
        gss = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=RNG)
        idx = np.arange(len(df))
        tr_idx, te_idx = next(gss.split(idx, groups=idx))
        tr, te = df.iloc[tr_idx].copy(), df.iloc[te_idx].copy()
        n_tr_group = len(tr)
        n_te_group = len(te)

    # --------------------------------------------
    # Scale covariates (fit on train, apply to test)
    # --------------------------------------------
    scaler = _fit_and_apply_scaler(tr, te, feats)
    _save_scaler(scaler, feats, name="aft_static")

    # --------------------------------------------
    # Define models to try
    # --------------------------------------------
    Fitters = _import_parametric_fitters()
    if len(Fitters) == 0:
        print("[NOTE] No parametric regression fitters available in your lifelines version.")
        return {}

    # Map to human-readable names and per-model penalizers if desired
    model_specs = []
    for key, Cls in Fitters.items():
        # lifelines AFT fitters accept 'penalizer' kwarg (L2); default 0 is fine.
        model_specs.append((key, Cls, {"penalizer": 0.0}))

    results = {}
    for key, Cls, kwargs in model_specs:
        name = f"aft_{key}"
        print(f"\n=== Fitting {name} ===")
        try:
            fitter = Cls(**kwargs)
            fitter.fit(
                tr[[DURATION_COL, EVENT_COL] + feats],
                duration_col=DURATION_COL,
                event_col=EVENT_COL,
                show_progress=False,
            )

            # Metrics: use negative predicted median survival time as "risk"
            tr_scores = _aft_risk_scores_from_median(fitter, tr[feats])
            te_scores = _aft_risk_scores_from_median(fitter, te[feats])
            tr_ci = concordance_index(tr[DURATION_COL], tr_scores, tr[EVENT_COL])
            te_ci = concordance_index(te[DURATION_COL], te_scores, te[EVENT_COL])

            # NEW: likelihood-based metrics (if exposed) + Brier/IBS via scikit-survival
            extra_metrics = {}
            extra_metrics.update(_extra_likelihood_metrics(fitter))
            extra_metrics.update(
                _survival_metrics_sksurv(
                    fitter,
                    tr,
                    te,
                    feats,
                    prefix=f"{name}_",
                )
            )



            # Save summary/model
            _save_parametric_summary(fitter, OUT_DIR / f"{name}_summary.csv")
            pickle_save(fitter, name)

            # Plots
            _plot_aft_coefficients(fitter, name)
            _plot_parametric_baselines(fitter, name)
            _plot_aft_individual_curves(fitter, tr, feats, name, "train")
            _plot_aft_individual_curves(fitter, te, feats, name, "test")

            # Info: AIC/BIC if present
            aic = getattr(fitter, "AIC_", None)
            bic = getattr(fitter, "BIC_", None)

            _print_metrics(f"{name} (AFT)", tr_ci, te_ci, n_tr_group, n_te_group)

            if "AIC" in extra_metrics:
                print(f"  AIC (train): {extra_metrics['AIC']:.3f}")
            if "BIC" in extra_metrics:
                print(f"  BIC (train): {extra_metrics['BIC']:.3f}")
            if f"{name}_ibs" in extra_metrics:
                print(f"  Test IBS: {extra_metrics[f'{name}_ibs']:.4f}")


            results[name] = {
                "train_cindex": float(tr_ci),
                "test_cindex": float(te_ci),
                "n_train": int(len(tr)),
                "n_test": int(len(te)),
                "n_train_groups": int(n_tr_group),
                "n_test_groups": int(n_te_group),
                "n_features": int(len(feats)),
                **extra_metrics,
            }

        except Exception as e:
            print(f"[ERROR] {name} failed to fit or evaluate: {e}")

    # Save consolidated metrics for parametrics
    try:
        with open(OUT_DIR / "metrics_parametric.json", "w") as f:
            json.dump(results, f, indent=2)
    except Exception as e:
        print(f"[WARN] Could not write metrics_parametric.json: {e}")

    return results


# ----------------------------------------------------
# Main
# ----------------------------------------------------
def main():
    results = {}
    results["coxph_static"] = run_static_cox(STATIC_PATH)

    # NEW: parametric AFT models on the same static dataset
    results["parametric_aft"] = run_parametric_aft_models(STATIC_PATH)

    try:
        pass
        #results["cox_tvc"] = run_timevarying_cox(TVC_PATH)
    except FileNotFoundError as e:
        print(f"\n[NOTE] Time-varying model not run: {e}\n")

    with open(OUT_DIR / "metrics.json", "w") as f:
        json.dump(results, f, indent=2)

    print(f"\n✅ Done. Summaries -> {OUT_DIR}/, metrics.json saved.")
    print("   Saved scalers -> outputs_cox/coxph_static_scaler.pkl, outputs_cox/aft_static_scaler.pkl and outputs_cox/cox_tvc_scaler.pkl")
    print(f"   Plots -> {PLOTS_DIR}/")
    if (OUT_DIR / "coxph_static_ph_test_summary.csv").exists():
        print("   PH test results (CoxPH):")
        print(f"     - {OUT_DIR / 'coxph_static_ph_test_summary.csv'}")
        print(f"     - {OUT_DIR / 'coxph_static_ph_check_report.txt'}")
        print(f"     - Figures in {PLOTS_DIR} with prefix 'coxph_static_ph_diag_'")


if __name__ == "__main__":
    main()
