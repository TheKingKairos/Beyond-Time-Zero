{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c02e55c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Sequence, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import os\n",
    "from dnn import GenericMLP, collate_dense_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d70bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, cross_validate, GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ba9c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['stay_id', 'temperature', 'heartrate', 'resprate', 'o2sat', 'sbp',\n",
       "       'dbp', 'pain', 'rhythm_flag', 'is_white', 'is_black', 'is_asian',\n",
       "       'is_hispanic', 'is_other_race', 'gender_F', 'gender_M',\n",
       "       'arrival_transport_AMBULANCE', 'arrival_transport_HELICOPTER',\n",
       "       'arrival_transport_OTHER', 'arrival_transport_UNKNOWN',\n",
       "       'arrival_transport_WALK IN', 'time_since_adm', 'gsn_16599.0',\n",
       "       'gsn_43952.0', 'gsn_4490.0', 'gsn_66419.0', 'gsn_61716.0', 'is_sepsis'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/MIMIC-ED/event_level_training_data.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63f1958c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stay_id                         0\n",
       "temperature                     0\n",
       "heartrate                       0\n",
       "resprate                        0\n",
       "o2sat                           0\n",
       "sbp                             0\n",
       "dbp                             0\n",
       "pain                            0\n",
       "rhythm_flag                     0\n",
       "is_white                        0\n",
       "is_black                        0\n",
       "is_asian                        0\n",
       "is_hispanic                     0\n",
       "is_other_race                   0\n",
       "gender_F                        0\n",
       "gender_M                        0\n",
       "arrival_transport_AMBULANCE     0\n",
       "arrival_transport_HELICOPTER    0\n",
       "arrival_transport_OTHER         0\n",
       "arrival_transport_UNKNOWN       0\n",
       "arrival_transport_WALK IN       0\n",
       "time_since_adm                  0\n",
       "gsn_16599.0                     0\n",
       "gsn_43952.0                     0\n",
       "gsn_4490.0                      0\n",
       "gsn_66419.0                     0\n",
       "gsn_61716.0                     0\n",
       "is_sepsis                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count nans\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70766e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay_id                           int64\n",
      "temperature                     float64\n",
      "heartrate                       float64\n",
      "resprate                        float64\n",
      "o2sat                           float64\n",
      "sbp                             float64\n",
      "dbp                             float64\n",
      "pain                            float64\n",
      "rhythm_flag                       int64\n",
      "is_white                          int64\n",
      "is_black                          int64\n",
      "is_asian                          int64\n",
      "is_hispanic                       int64\n",
      "is_other_race                     int64\n",
      "gender_F                          int64\n",
      "gender_M                          int64\n",
      "arrival_transport_AMBULANCE       int64\n",
      "arrival_transport_HELICOPTER      int64\n",
      "arrival_transport_OTHER           int64\n",
      "arrival_transport_UNKNOWN         int64\n",
      "arrival_transport_WALK IN         int64\n",
      "time_since_adm                  float64\n",
      "gsn_16599.0                       int64\n",
      "gsn_43952.0                       int64\n",
      "gsn_4490.0                        int64\n",
      "gsn_66419.0                       int64\n",
      "gsn_61716.0                       int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# do gssplit with this small df\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "y = df[\"is_sepsis\"]\n",
    "X = df.drop(columns=[\"is_sepsis\"])\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=df[\"stay_id\"]))\n",
    "x_train, x_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "# print column types in X\n",
    "print(x_train.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dce81a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total configs: 288\n",
      "\n",
      "[1/288] Config: {'hidden_sizes': (128, 64), 'dropout': 0.0, 'activation': 'relu', 'batch_norm': False, 'final_dropout': 0.0, 'lr': 0.001, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1393866/1431526782.py:241: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(cfg.amp and device.type == \"cuda\"))\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/ykim3041/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ykim3041/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'FastTabularDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 498\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[32m    495\u001b[39m \u001b[38;5;66;03m# (Optional) Quick single run\u001b[39;00m\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# ============================\u001b[39;00m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m498\u001b[39m     \u001b[43mrun_grid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbest_generic_mlp.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 439\u001b[39m, in \u001b[36mrun_grid_search\u001b[39m\u001b[34m(save_path)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_configs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg_dict\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    437\u001b[39m model = _rebuild_model(input_size, cfg_dict)\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m best_val, metrics, best_state = \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptim_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m result = GridResult(val_loss=best_val, metrics=metrics, config=cfg_dict, state_dict=best_state)\n\u001b[32m    448\u001b[39m results.append(result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 261\u001b[39m, in \u001b[36mtrain_and_validate\u001b[39m\u001b[34m(model, train_loader, val_loader, cfg, optim_kwargs)\u001b[39m\n\u001b[32m    258\u001b[39m train_loss_sum = \u001b[32m0.0\u001b[39m\n\u001b[32m    259\u001b[39m n_train = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mset_to_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/septic6/sepsis/lib/python3.12/site-packages/torch/utils/data/dataloader.py:488\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.persistent_workers \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_workers > \u001b[32m0\u001b[39m:\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m         \u001b[38;5;28mself\u001b[39m._iterator = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    490\u001b[39m         \u001b[38;5;28mself\u001b[39m._iterator._reset(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/septic6/sepsis/lib/python3.12/site-packages/torch/utils/data/dataloader.py:424\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/septic6/sepsis/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1171\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1164\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1170\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1173\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/multiprocessing/process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/multiprocessing/context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/multiprocessing/context.py:289\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/multiprocessing/popen_spawn_posix.py:32\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m._fds = []\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.returncode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/multiprocessing/popen_spawn_posix.py:62\u001b[39m, in \u001b[36mPopen._launch\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.sentinel = parent_r\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m, closefd=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     64\u001b[39m     fds_to_close = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Grid Search for GenericMLP (CUDA 12.1 optimized) + Save Best\n",
    "# ============================\n",
    "import os, math, random, itertools, json, datetime\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Tuple, Sequence, NamedTuple, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # ✅ CUDA 12.1/Ampere+ performance knobs\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True  # TF32 for matmuls (A100/RTX30+)\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        # Prefer HFMA/TF32 kernels when applicable\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    batch_size: int = 2048\n",
    "    max_epochs: int = 30\n",
    "    patience: int = 4\n",
    "    grad_clip: float = 1.0\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    loss_type: str = \"bce\"          # \"bce\" or \"mse\"\n",
    "    val_split: float = 0.2          # used only if we fall back to (X, y)\n",
    "    num_workers: int = max(4, (os.cpu_count() or 8) // 2)\n",
    "    prefetch_factor: int = 2\n",
    "    amp: bool = True\n",
    "    compile: bool = True            # ✅ will try torch.compile on CUDA\n",
    "    use_fused_adamw: bool = True    # ✅ use fused AdamW when available\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GridResult:\n",
    "    val_loss: float\n",
    "    metrics: Dict[str, float]\n",
    "    config: Dict[str, object]\n",
    "    state_dict: Dict[str, torch.Tensor]  # ✅ keep best weights for this config\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset (Fast path)\n",
    "# -----------------------------\n",
    "class FastTabularDataset(Dataset):\n",
    "    \"\"\"Stores full feature/target tensors (CPU) and slices by index.\"\"\"\n",
    "    def __init__(self, features_2d: torch.Tensor, targets_1d: torch.Tensor):\n",
    "        assert features_2d.ndim == 2, \"features must be 2D (N, D)\"\n",
    "        assert targets_1d.ndim == 1, \"targets must be 1D (N,)\"\n",
    "        assert features_2d.size(0) == targets_1d.size(0)\n",
    "        self.x = features_2d.contiguous()\n",
    "        self.y = targets_1d.contiguous()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.x.size(0)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class Batch(NamedTuple):\n",
    "    x: torch.Tensor  # (B, D)\n",
    "    y: torch.Tensor  # (B, 1)\n",
    "\n",
    "def collate_dense_batch(samples: Sequence[Tuple[torch.Tensor, torch.Tensor]]) -> Batch:\n",
    "    xs, ys = zip(*samples)\n",
    "    x = torch.stack(xs, dim=0)\n",
    "    y = torch.as_tensor(ys, dtype=torch.float32).view(-1, 1)\n",
    "    return Batch(x=x, y=y)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Model\n",
    "# -----------------------------\n",
    "_ACTS: Dict[str, nn.Module] = {\n",
    "    \"relu\": nn.ReLU(),\n",
    "    \"gelu\": nn.GELU(approximate=\"tanh\"),\n",
    "    \"silu\": nn.SiLU()\n",
    "}\n",
    "\n",
    "class GenericMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_sizes: Tuple[int, ...] = (256, 128),\n",
    "        dropout: float = 0.0,\n",
    "        activation: str = \"relu\",\n",
    "        batch_norm: bool = False,\n",
    "        final_dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        act = _ACTS[activation]\n",
    "        layers: List[nn.Module] = []\n",
    "        prev = input_size\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev, h, bias=not batch_norm))\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(act)\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "        self.head = nn.Linear(prev, 1)\n",
    "        self.final_dropout = nn.Dropout(final_dropout) if final_dropout > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, return_logits: bool = True) -> torch.Tensor:\n",
    "        z = self.backbone(x)\n",
    "        z = self.final_dropout(z)\n",
    "        logits = self.head(z)\n",
    "        if return_logits:\n",
    "            return logits\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Data loading (uses your pre-split, if present)\n",
    "# -----------------------------\n",
    "def make_fast_dataset_from_xy(X_np: np.ndarray, y_np: np.ndarray) -> FastTabularDataset:\n",
    "    f = torch.tensor(X_np, dtype=torch.float32)\n",
    "    t = torch.tensor(y_np, dtype=torch.float32)\n",
    "    return FastTabularDataset(f, t)\n",
    "\n",
    "def _maybe_use_presplit() -> Optional[FastTabularDataset]:\n",
    "    \"\"\"\n",
    "    If x_train/y_train are defined in the notebook (as you mentioned),\n",
    "    use them; otherwise return None and we'll fall back to full X,y.\n",
    "    \"\"\"\n",
    "    g = globals()\n",
    "    if all(k in g for k in (\"x_train\", \"y_train\")):\n",
    "        X_np = g[\"x_train\"].to_numpy() if hasattr(g[\"x_train\"], \"to_numpy\") else np.asarray(g[\"x_train\"])\n",
    "        y_np = g[\"y_train\"].to_numpy() if hasattr(g[\"y_train\"], \"to_numpy\") else np.asarray(g[\"y_train\"])\n",
    "        return make_fast_dataset_from_xy(X_np, y_np)\n",
    "    return None\n",
    "\n",
    "def _fallback_full_dataset() -> FastTabularDataset:\n",
    "    # Assumes pandas DataFrames/Series X, y exist (your original code).\n",
    "    g = globals()\n",
    "    X = g[\"X\"]; y = g[\"y\"]\n",
    "    features_2d = torch.tensor(X.drop(columns=[\"stay_id\"]).to_numpy(), dtype=torch.float32)\n",
    "    targets_1d = torch.tensor(y.to_numpy(), dtype=torch.float32)\n",
    "    return FastTabularDataset(features_2d, targets_1d)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Loaders\n",
    "# -----------------------------\n",
    "def make_loaders(ds: Dataset, cfg: TrainConfig) -> Tuple[DataLoader, DataLoader]:\n",
    "    # When pre-split is used, ds is already \"train only\"; we'll still carve a val split from it.\n",
    "    n_val = max(1, int(len(ds) * cfg.val_split))\n",
    "    n_train = len(ds) - n_val\n",
    "    train_ds, val_ds = random_split(ds, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "    pin = (cfg.device == \"cuda\")\n",
    "    nw = cfg.num_workers if len(ds) > 1024 else 0\n",
    "    pw = bool(nw > 0)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_dense_batch,\n",
    "        num_workers=nw,\n",
    "        pin_memory=pin,\n",
    "        persistent_workers=pw,\n",
    "        prefetch_factor=cfg.prefetch_factor if nw > 0 else None,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_dense_batch,\n",
    "        num_workers=nw,\n",
    "        pin_memory=pin,\n",
    "        persistent_workers=pw,\n",
    "        prefetch_factor=cfg.prefetch_factor if nw > 0 else None,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training / Evaluation\n",
    "# -----------------------------\n",
    "def _best_amp_dtype() -> torch.dtype:\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "        return torch.bfloat16\n",
    "    return torch.float16\n",
    "\n",
    "def _make_optimizer(params, cfg: TrainConfig, lr: float, weight_decay: float):\n",
    "    # ✅ Prefer fused AdamW on CUDA if available (PyTorch 2.0+)\n",
    "    use_fused = (cfg.use_fused_adamw and torch.cuda.is_available())\n",
    "    try:\n",
    "        if use_fused:\n",
    "            return torch.optim.AdamW(params, lr=lr, weight_decay=weight_decay, fused=True)\n",
    "    except TypeError:\n",
    "        pass\n",
    "    return torch.optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "def train_and_validate(\n",
    "    model: GenericMLP,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    cfg: TrainConfig,\n",
    "    optim_kwargs: Dict,\n",
    ") -> Tuple[float, Dict[str, float], Dict[str, torch.Tensor]]:\n",
    "    device = torch.device(cfg.device)\n",
    "    model.to(device)\n",
    "\n",
    "    # Loss\n",
    "    if cfg.loss_type == \"bce\":\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        use_logits = True\n",
    "    elif cfg.loss_type == \"mse\":\n",
    "        criterion = nn.MSELoss()\n",
    "        use_logits = False\n",
    "    else:\n",
    "        raise ValueError(\"loss_type must be 'bce' or 'mse'\")\n",
    "\n",
    "    optimizer = _make_optimizer(model.parameters(), cfg, lr=optim_kwargs[\"lr\"], weight_decay=optim_kwargs[\"weight_decay\"])\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(cfg.amp and device.type == \"cuda\"))\n",
    "    amp_dtype = _best_amp_dtype()\n",
    "\n",
    "    # ✅ Optional compile (PyTorch 2.x)\n",
    "    if cfg.compile and hasattr(torch, \"compile\") and device.type == \"cuda\":\n",
    "        try:\n",
    "            model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    best_val_loss = math.inf\n",
    "    best_metrics: Dict[str, float] = {}\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(cfg.max_epochs):\n",
    "        model.train()\n",
    "        train_loss_sum = 0.0\n",
    "        n_train = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            x, y = batch.x.to(device, non_blocking=True), batch.y.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if cfg.amp and device.type == \"cuda\":\n",
    "                with torch.cuda.amp.autocast(dtype=amp_dtype):\n",
    "                    if use_logits:\n",
    "                        logits = model(x, return_logits=True)\n",
    "                        loss = criterion(logits, y)\n",
    "                    else:\n",
    "                        probs = model(x, return_logits=False)\n",
    "                        loss = criterion(probs, y)\n",
    "                scaler.scale(loss).backward()\n",
    "                if cfg.grad_clip is not None:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                if use_logits:\n",
    "                    logits = model(x, return_logits=True)\n",
    "                    loss = criterion(logits, y)\n",
    "                else:\n",
    "                    probs = model(x, return_logits=False)\n",
    "                    loss = criterion(probs, y)\n",
    "                loss.backward()\n",
    "                if cfg.grad_clip is not None:\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "                optimizer.step()\n",
    "\n",
    "            bs = x.size(0)\n",
    "            train_loss_sum += float(loss) * bs\n",
    "            n_train += bs\n",
    "\n",
    "        train_loss = train_loss_sum / max(1, n_train)\n",
    "\n",
    "        # ---- Validation (fast path) ----\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        n_val = 0\n",
    "        correct = 0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            ctx = torch.cuda.amp.autocast(dtype=amp_dtype) if (cfg.amp and device.type == \"cuda\") else torch.no_grad()\n",
    "            with ctx:\n",
    "                for batch in val_loader:\n",
    "                    x, y = batch.x.to(device, non_blocking=True), batch.y.to(device, non_blocking=True)\n",
    "                    if use_logits:\n",
    "                        logits = model(x, return_logits=True)\n",
    "                        loss = criterion(logits, y)\n",
    "                        probs = torch.sigmoid(logits)\n",
    "                    else:\n",
    "                        probs = model(x, return_logits=False)\n",
    "                        loss = criterion(probs, y)\n",
    "                    bs = x.size(0)\n",
    "                    val_loss_sum += float(loss) * bs\n",
    "                    n_val += bs\n",
    "                    preds = (probs >= 0.5).to(y.dtype)\n",
    "                    correct += (preds == (y >= 0.5)).sum().item()\n",
    "\n",
    "        val_loss = val_loss_sum / max(1, n_val)\n",
    "        val_acc = correct / max(1, n_val)\n",
    "\n",
    "        # ---- Early stopping ----\n",
    "        if val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = val_loss\n",
    "            best_metrics = {\"val_loss\": val_loss, \"val_acc\": val_acc, \"train_loss\": train_loss}\n",
    "            epochs_no_improve = 0\n",
    "            # ✅ keep raw (uncompiled) weights on CPU\n",
    "            state_cpu = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            best_state = state_cpu\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= cfg.patience:\n",
    "                break\n",
    "\n",
    "    if best_state is None:\n",
    "        raise RuntimeError(\"Training completed without capturing a best state.\")\n",
    "    return best_val_loss, best_metrics, best_state\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Grid search (+ save best model)\n",
    "# -----------------------------\n",
    "def _rebuild_model(input_size: int, cfg_dict: Dict[str, object]) -> GenericMLP:\n",
    "    return GenericMLP(\n",
    "        input_size=input_size,\n",
    "        hidden_sizes=tuple(cfg_dict[\"hidden_sizes\"]),\n",
    "        dropout=float(cfg_dict[\"dropout\"]),\n",
    "        activation=str(cfg_dict[\"activation\"]),\n",
    "        batch_norm=bool(cfg_dict[\"batch_norm\"]),\n",
    "        final_dropout=float(cfg_dict[\"final_dropout\"]),\n",
    "    )\n",
    "\n",
    "def _save_best_model(\n",
    "    filepath: str,\n",
    "    input_size: int,\n",
    "    best_cfg: Dict[str, object],\n",
    "    best_state: Dict[str, torch.Tensor],\n",
    "    metrics: Dict[str, float],\n",
    "    val_loss: float,\n",
    "    train_cfg: TrainConfig,\n",
    "):\n",
    "    payload = {\n",
    "        \"model_class\": \"GenericMLP\",\n",
    "        \"model_kwargs\": {\n",
    "            \"input_size\": int(input_size),\n",
    "            \"hidden_sizes\": tuple(best_cfg[\"hidden_sizes\"]),\n",
    "            \"dropout\": float(best_cfg[\"dropout\"]),\n",
    "            \"activation\": str(best_cfg[\"activation\"]),\n",
    "            \"batch_norm\": bool(best_cfg[\"batch_norm\"]),\n",
    "            \"final_dropout\": float(best_cfg[\"final_dropout\"]),\n",
    "        },\n",
    "        \"state_dict\": best_state,\n",
    "        \"grid_config\": {k: (tuple(v) if isinstance(v, list) else v) for k, v in best_cfg.items()},\n",
    "        \"metrics\": metrics,\n",
    "        \"val_loss\": float(val_loss),\n",
    "        \"train_config\": asdict(train_cfg),\n",
    "        \"created\": datetime.datetime.now().isoformat(),\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else None,\n",
    "        \"device_saved\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    }\n",
    "    torch.save(payload, filepath)\n",
    "\n",
    "    # ✅ Optional TorchScript export for deployment\n",
    "    try:\n",
    "        model = _rebuild_model(input_size, best_cfg)\n",
    "        model.load_state_dict(best_state, strict=True)\n",
    "        model.eval()\n",
    "        example = torch.randn(1, input_size)\n",
    "        scripted = torch.jit.trace(model, example)\n",
    "        torch.jit.save(scripted, os.path.splitext(filepath)[0] + \"_scripted.pt\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def run_grid_search(save_path: str = \"best_generic_mlp.pt\"):\n",
    "    print(\"Using device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    seed_everything(7)\n",
    "\n",
    "    # Prefer your pre-split x_train/y_train if present\n",
    "    ds = _maybe_use_presplit()\n",
    "    if ds is None:\n",
    "        ds = _fallback_full_dataset()\n",
    "\n",
    "    train_cfg = TrainConfig()\n",
    "    train_loader, val_loader = make_loaders(ds, train_cfg)\n",
    "    input_size = ds.x.size(-1)\n",
    "\n",
    "    param_grid = {\n",
    "        \"hidden_sizes\": [\n",
    "            (128, 64),\n",
    "            (256, 128),\n",
    "            (256, 128, 64),\n",
    "        ],\n",
    "        \"dropout\": [0.0, 0.1, 0.25],\n",
    "        \"activation\": [\"relu\", \"gelu\"],\n",
    "        \"batch_norm\": [False, True],\n",
    "        \"final_dropout\": [0.0, 0.1],\n",
    "        \"lr\": [1e-3, 5e-4],\n",
    "        \"weight_decay\": [0.0, 1e-4],\n",
    "    }\n",
    "\n",
    "    keys = list(param_grid.keys())\n",
    "    grid_values = [param_grid[k] for k in keys]\n",
    "    total_configs = math.prod(len(v) for v in grid_values)\n",
    "\n",
    "    results: List[GridResult] = []\n",
    "    best_overall: Optional[GridResult] = None  # ✅ track winner across grid\n",
    "\n",
    "    print(f\"Total configs: {total_configs}\")\n",
    "\n",
    "    for i, values in enumerate(itertools.product(*grid_values), 1):\n",
    "        cfg_dict = dict(zip(keys, values))\n",
    "        print(f\"\\n[{i}/{total_configs}] Config: {cfg_dict}\")\n",
    "\n",
    "        model = _rebuild_model(input_size, cfg_dict)\n",
    "\n",
    "        best_val, metrics, best_state = train_and_validate(\n",
    "            model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            cfg=train_cfg,\n",
    "            optim_kwargs={\"lr\": cfg_dict[\"lr\"], \"weight_decay\": cfg_dict[\"weight_decay\"]},\n",
    "        )\n",
    "\n",
    "        result = GridResult(val_loss=best_val, metrics=metrics, config=cfg_dict, state_dict=best_state)\n",
    "        results.append(result)\n",
    "        print(f\"=> best_val_loss={best_val:.4f}, val_acc={metrics['val_acc']:.4f}\")\n",
    "\n",
    "        # ✅ Maintain global best\n",
    "        if (best_overall is None) or (best_val < best_overall.val_loss - 1e-12):\n",
    "            best_overall = result\n",
    "\n",
    "    # Sort for leaderboard\n",
    "    results.sort(key=lambda r: r.val_loss)\n",
    "\n",
    "    print(\"\\n===== Leaderboard (by val_loss) =====\")\n",
    "    for rank, result in enumerate(results[:10], 1):\n",
    "        val_loss = result.val_loss\n",
    "        metrics = result.metrics\n",
    "        cfg_dict = result.config\n",
    "        print(\n",
    "            f\"#{rank:>2} val_loss={val_loss:.4f} val_acc={metrics['val_acc']:.4f} \"\n",
    "            f\"train_loss={metrics['train_loss']:.4f} | {cfg_dict}\"\n",
    "        )\n",
    "\n",
    "    # ✅ Save the single best model across the grid\n",
    "    assert best_overall is not None\n",
    "    print(f\"\\nSaving best model to: {save_path}\")\n",
    "    _save_best_model(\n",
    "        filepath=save_path,\n",
    "        input_size=input_size,\n",
    "        best_cfg=best_overall.config,\n",
    "        best_state=best_overall.state_dict,\n",
    "        metrics=best_overall.metrics,\n",
    "        val_loss=best_overall.val_loss,\n",
    "        train_cfg=train_cfg,\n",
    "    )\n",
    "\n",
    "    # Report best config/metrics\n",
    "    print(\"\\nBest Config:\")\n",
    "    for k, v in best_overall.config.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(\"Best Metrics:\")\n",
    "    for k, v in best_overall.metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    print(f\"Saved weights and metadata to: {save_path}\")\n",
    "    ts_path = os.path.splitext(save_path)[0] + \"_scripted.pt\"\n",
    "    if os.path.exists(ts_path):\n",
    "        print(f\"(TorchScript export available at: {ts_path})\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# (Optional) Quick single run\n",
    "# ============================\n",
    "if __name__ == \"__main__\":\n",
    "    run_grid_search(\"best_generic_mlp.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575744c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sepsis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
